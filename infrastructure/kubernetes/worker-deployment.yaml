# =============================================================================
# PULL Temporal Worker Deployment - Production Configuration
# =============================================================================
# This file contains all resources needed to run Temporal workers:
#   - Deployment: Worker pods that process Temporal workflows and activities
#   - Service: Headless service for health checks and metrics
#   - HorizontalPodAutoscaler: Scales based on task queue depth
#   - PodDisruptionBudget: Ensures availability during disruptions
#   - VerticalPodAutoscaler: Resource right-sizing recommendations
#
# Temporal Workers:
#   - Poll Temporal server for workflow/activity tasks
#   - Execute business logic (Plaid, Kalshi, Polymarket integrations)
#   - Require graceful shutdown for long-running activities
#   - Scale based on task queue backlog, not just CPU/memory
#
# Prerequisites:
#   - namespace.yaml (namespace and quotas)
#   - configmaps.yaml (configuration)
#   - external-secrets.yaml or secrets (API credentials)
#   - service-accounts.yaml (RBAC)
#   - priority-classes.yaml (scheduling priority)
# =============================================================================

---
# -----------------------------------------------------------------------------
# Temporal Worker Deployment
# -----------------------------------------------------------------------------
# Workers process background tasks from Temporal task queues.
#
# Key characteristics:
#   - Long-running: Activities may take minutes to complete
#   - Stateless: State is stored in Temporal, not in worker pods
#   - Resource-intensive: Higher CPU/memory than API pods
#   - Graceful shutdown: Must complete in-flight activities before termination
#
# Scaling strategy:
#   - Minimum 5 replicas for throughput and resilience
#   - HPA scales based on task queue backlog metrics
#   - Conservative scale-down to avoid interrupting activities
# -----------------------------------------------------------------------------
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pull-temporal-worker
  namespace: pull
  labels:
    # Standard Kubernetes labels
    app: pull-temporal-worker
    app.kubernetes.io/name: pull-temporal-worker
    app.kubernetes.io/instance: pull-temporal-worker-production
    app.kubernetes.io/component: worker
    app.kubernetes.io/part-of: pull
    app.kubernetes.io/version: "1.0.0"
    app.kubernetes.io/managed-by: kubectl
    # Custom labels
    tier: backend
    environment: production
  annotations:
    kubernetes.io/change-cause: "Initial production deployment"
    description: "Temporal workers processing background workflows and activities"
spec:
  # Initial replica count (managed by HPA in production)
  # 5 replicas provide good throughput and redundancy
  replicas: 5

  # Keep revision history for rollbacks
  revisionHistoryLimit: 10

  selector:
    matchLabels:
      app: pull-temporal-worker

  # Rolling update with tolerance for some unavailability
  # Workers can handle some disruption since Temporal retries
  strategy:
    type: RollingUpdate
    rollingUpdate:
      # Allow 2 extra pods during updates for faster rollout
      maxSurge: 2
      # Allow 1 pod to be unavailable during updates
      # Temporal will retry any interrupted activities
      maxUnavailable: 1

  # Progress deadline
  progressDeadlineSeconds: 600

  # Wait longer for workers to become ready
  minReadySeconds: 15

  template:
    metadata:
      labels:
        app: pull-temporal-worker
        app.kubernetes.io/name: pull-temporal-worker
        app.kubernetes.io/instance: pull-temporal-worker-production
        app.kubernetes.io/component: worker
        app.kubernetes.io/part-of: pull
        version: v1
      annotations:
        # Prometheus scraping
        prometheus.io/scrape: "true"
        prometheus.io/port: "3002"
        prometheus.io/path: "/metrics"

        # Config checksum for rollout on config change
        checksum/config: "${CONFIG_CHECKSUM}"

        # Security
        seccomp.security.alpha.kubernetes.io/pod: runtime/default

        # Cluster autoscaler
        cluster-autoscaler.kubernetes.io/safe-to-evict: "true"

    spec:
      # -----------------------------------------------------------------------
      # Service Account
      # -----------------------------------------------------------------------
      serviceAccountName: pull-worker
      automountServiceAccountToken: false

      # -----------------------------------------------------------------------
      # Termination Configuration
      # -----------------------------------------------------------------------
      # CRITICAL: Workers need long grace period to complete activities
      # terminationGracePeriodSeconds should be >= longest activity timeout
      # Our longest activity (market sync) can take up to 60s
      # preStop hook adds 90s buffer, so total: 90 + 30 = 120s minimum
      terminationGracePeriodSeconds: 120

      # -----------------------------------------------------------------------
      # Pod Security Context
      # -----------------------------------------------------------------------
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        runAsGroup: 1000
        fsGroup: 1000
        fsGroupChangePolicy: OnRootMismatch
        seccompProfile:
          type: RuntimeDefault
        supplementalGroups: [1000]

      # -----------------------------------------------------------------------
      # Scheduling
      # -----------------------------------------------------------------------
      # Workers have slightly lower priority than API
      priorityClassName: pull-worker-priority

      dnsPolicy: ClusterFirst
      dnsConfig:
        options:
          - name: ndots
            value: "2"
          - name: single-request-reopen

      restartPolicy: Always

      # -----------------------------------------------------------------------
      # Container
      # -----------------------------------------------------------------------
      containers:
        - name: worker
          image: gcr.io/PROJECT_ID/pull-temporal-worker:latest
          imagePullPolicy: Always

          # -----------------------------------------------------------------
          # Ports
          # -----------------------------------------------------------------
          ports:
            # Health check and debug port
            - containerPort: 3002
              name: health
              protocol: TCP
            # Prometheus metrics
            - containerPort: 9090
              name: metrics
              protocol: TCP

          # -----------------------------------------------------------------
          # Environment
          # -----------------------------------------------------------------
          envFrom:
            - configMapRef:
                name: pull-shared-config
            - configMapRef:
                name: pull-worker-config
            - configMapRef:
                name: pull-observability-config
            - configMapRef:
                name: pull-redis-config

          env:
            # Pod metadata
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            - name: POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName

            # Temporal connection
            - name: TEMPORAL_ADDRESS
              value: "temporal:7233"

            # Database/Cache credentials
            - name: CONVEX_URL
              valueFrom:
                secretKeyRef:
                  name: pull-secrets
                  key: convex-url
            - name: CONVEX_DEPLOY_KEY
              valueFrom:
                secretKeyRef:
                  name: pull-secrets
                  key: convex-deploy-key
            - name: REDIS_URL
              valueFrom:
                secretKeyRef:
                  name: pull-secrets
                  key: redis-url

            # External API credentials
            - name: KALSHI_API_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: pull-secrets
                  key: kalshi-api-key-id
            - name: KALSHI_PRIVATE_KEY
              valueFrom:
                secretKeyRef:
                  name: pull-secrets
                  key: kalshi-private-key
            - name: PLAID_CLIENT_ID
              valueFrom:
                secretKeyRef:
                  name: pull-secrets
                  key: plaid-client-id
            - name: PLAID_SECRET
              valueFrom:
                secretKeyRef:
                  name: pull-secrets
                  key: plaid-secret

            # Encryption
            - name: ENCRYPTION_KEY
              valueFrom:
                secretKeyRef:
                  name: pull-secrets
                  key: encryption-key

          # -----------------------------------------------------------------
          # Resources
          # -----------------------------------------------------------------
          # Workers need more resources than API:
          # - CPU: For data processing and API calls
          # - Memory: For caching and batch operations
          # - Storage: For workflow data and logs
          resources:
            requests:
              cpu: "500m"           # 0.5 CPU cores
              memory: "1Gi"         # 1 GiB memory
              ephemeral-storage: "200Mi"
            limits:
              cpu: "2000m"          # Can burst to 2 CPU cores
              memory: "4Gi"         # 4 GiB hard limit
              ephemeral-storage: "1Gi"

          # -----------------------------------------------------------------
          # Health Probes
          # -----------------------------------------------------------------
          # Startup: Allow longer initialization for Temporal connection
          startupProbe:
            httpGet:
              path: /health
              port: 3002
              httpHeaders:
                - name: X-Health-Check
                  value: startup
            initialDelaySeconds: 10
            periodSeconds: 5
            timeoutSeconds: 10
            # Workers may take up to 3 minutes to start (36 * 5s)
            failureThreshold: 36
            successThreshold: 1

          # Readiness: Check if worker can accept tasks
          # Should verify Temporal connection is healthy
          readinessProbe:
            httpGet:
              path: /health/ready
              port: 3002
              httpHeaders:
                - name: X-Health-Check
                  value: readiness
            initialDelaySeconds: 0
            periodSeconds: 10
            timeoutSeconds: 5
            failureThreshold: 3
            successThreshold: 1

          # Liveness: Restart if worker is fundamentally broken
          livenessProbe:
            httpGet:
              path: /health/live
              port: 3002
              httpHeaders:
                - name: X-Health-Check
                  value: liveness
            initialDelaySeconds: 0
            periodSeconds: 30
            timeoutSeconds: 10
            failureThreshold: 3
            successThreshold: 1

          # -----------------------------------------------------------------
          # Lifecycle
          # -----------------------------------------------------------------
          lifecycle:
            preStop:
              exec:
                command:
                  - /bin/sh
                  - -c
                  - |
                    # Graceful shutdown for Temporal workers
                    # This is CRITICAL - incomplete activities will be retried
                    # but we want to avoid unnecessary retries

                    echo "[$(date)] Starting graceful shutdown..."

                    # 1. Signal worker to stop polling for new tasks
                    # The application should handle SIGTERM by calling
                    # worker.stop() which stops polling but completes
                    # in-flight activities
                    kill -TERM 1 2>/dev/null || true

                    # 2. Wait for in-flight activities to complete
                    # Maximum activity timeout is 5 minutes (300s)
                    # We wait 90 seconds here, allowing most activities
                    # to complete. The remaining 30s is for cleanup.
                    echo "[$(date)] Waiting for activities to complete..."
                    sleep 90

                    echo "[$(date)] Graceful shutdown complete"

          # -----------------------------------------------------------------
          # Security
          # -----------------------------------------------------------------
          securityContext:
            readOnlyRootFilesystem: true
            allowPrivilegeEscalation: false
            runAsNonRoot: true
            runAsUser: 1000
            capabilities:
              drop:
                - ALL

          # -----------------------------------------------------------------
          # Volume Mounts
          # -----------------------------------------------------------------
          volumeMounts:
            # Required for read-only root fs
            - name: tmp
              mountPath: /tmp
            # Node.js cache
            - name: cache
              mountPath: /home/node/.cache
            # Workflow data (intermediate results, large payloads)
            - name: workflow-data
              mountPath: /data/workflows

      # -----------------------------------------------------------------------
      # Volumes
      # -----------------------------------------------------------------------
      volumes:
        - name: tmp
          emptyDir:
            sizeLimit: 200Mi
        - name: cache
          emptyDir:
            sizeLimit: 500Mi
        - name: workflow-data
          emptyDir:
            sizeLimit: 1Gi

      # -----------------------------------------------------------------------
      # Affinity Rules
      # -----------------------------------------------------------------------
      affinity:
        # Spread workers across nodes and zones
        podAntiAffinity:
          # Hard: Never on same node
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app
                    operator: In
                    values:
                      - pull-temporal-worker
              topologyKey: kubernetes.io/hostname
          # Soft: Try different zones
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              podAffinityTerm:
                labelSelector:
                  matchExpressions:
                    - key: app
                      operator: In
                      values:
                        - pull-temporal-worker
                topologyKey: topology.kubernetes.io/zone

        # Prefer nodes with more resources (for CPU-intensive activities)
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 50
              preference:
                matchExpressions:
                  - key: node.kubernetes.io/instance-type
                    operator: In
                    values:
                      - n2-standard-4
                      - n2-standard-8
                      - n2-highmem-4
                      - n2-highmem-8
                      - e2-standard-4
                      - e2-standard-8

      # -----------------------------------------------------------------------
      # Topology Spread
      # -----------------------------------------------------------------------
      topologySpreadConstraints:
        # Spread across zones (allow some skew for workers)
        - maxSkew: 2
          topologyKey: topology.kubernetes.io/zone
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app: pull-temporal-worker
        # Spread across nodes
        - maxSkew: 2
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: ScheduleAnyway
          labelSelector:
            matchLabels:
              app: pull-temporal-worker

---
# -----------------------------------------------------------------------------
# Worker Service (Headless)
# -----------------------------------------------------------------------------
# Headless service for:
#   - Health check endpoints
#   - Prometheus metrics scraping
#   - Direct pod access for debugging
#
# Workers don't need load balancing since they poll Temporal.
# -----------------------------------------------------------------------------
apiVersion: v1
kind: Service
metadata:
  name: pull-temporal-worker
  namespace: pull
  labels:
    app: pull-temporal-worker
    app.kubernetes.io/name: pull-temporal-worker
    app.kubernetes.io/component: worker
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "9090"
    description: "Headless service for Temporal worker health checks and metrics"
spec:
  type: ClusterIP
  # Headless service for direct pod access
  clusterIP: None
  ports:
    - port: 3002
      targetPort: 3002
      protocol: TCP
      name: health
    - port: 9090
      targetPort: 9090
      protocol: TCP
      name: metrics
  selector:
    app: pull-temporal-worker

---
# -----------------------------------------------------------------------------
# Horizontal Pod Autoscaler
# -----------------------------------------------------------------------------
# Scales workers based on:
#   1. CPU/Memory utilization
#   2. Temporal task queue backlog (requires Prometheus Adapter)
#
# Queue-based scaling is preferred since it directly reflects work to be done.
# -----------------------------------------------------------------------------
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: pull-temporal-worker-hpa
  namespace: pull
  labels:
    app: pull-temporal-worker
  annotations:
    description: "HPA for Temporal workers - scales 5-50 pods based on queue depth"
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: pull-temporal-worker

  # Replica bounds
  minReplicas: 5    # Minimum for throughput
  maxReplicas: 50   # Maximum based on quota and cost

  metrics:
    # CPU utilization
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

    # Memory utilization
    - type: Resource
      resource:
        name: memory
        target:
          type: Utilization
          averageUtilization: 75

    # Workflow task queue backlog (Temporal metric)
    # Requires prometheus-adapter configured with Temporal metrics
    - type: External
      external:
        metric:
          name: temporal_workflow_task_queue_backlog
          selector:
            matchLabels:
              task_queue: pull-main-queue
        target:
          type: AverageValue
          # Scale up when backlog > 10 tasks per worker
          averageValue: "10"

    # Activity task queue backlog
    - type: External
      external:
        metric:
          name: temporal_activity_task_queue_backlog
          selector:
            matchLabels:
              task_queue: pull-main-queue
        target:
          type: AverageValue
          # Scale up when backlog > 50 activities per worker
          averageValue: "50"

  behavior:
    # Conservative scale-down for workers
    # Avoid interrupting activities with rapid scale-down
    scaleDown:
      # Wait 10 minutes before scale-down
      stabilizationWindowSeconds: 600
      policies:
        # Remove max 10% every 2 minutes
        - type: Percent
          value: 10
          periodSeconds: 120
        # Remove max 2 pods every 2 minutes
        - type: Pods
          value: 2
          periodSeconds: 120
      selectPolicy: Min

    # Aggressive scale-up when queue builds up
    scaleUp:
      stabilizationWindowSeconds: 30
      policies:
        # Add up to 50% more every 30 seconds
        - type: Percent
          value: 50
          periodSeconds: 30
        # Add up to 5 pods every 30 seconds
        - type: Pods
          value: 5
          periodSeconds: 30
      selectPolicy: Max

---
# -----------------------------------------------------------------------------
# Pod Disruption Budget
# -----------------------------------------------------------------------------
# Ensures enough workers are always available.
# Workers are more tolerant to disruption than API since Temporal retries.
# -----------------------------------------------------------------------------
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: pull-temporal-worker-pdb
  namespace: pull
  labels:
    app: pull-temporal-worker
  annotations:
    description: "Ensures minimum 3 workers during disruptions"
spec:
  # At least 3 workers must be available
  # Allows for faster node drains than API
  minAvailable: 3
  selector:
    matchLabels:
      app: pull-temporal-worker

---
# -----------------------------------------------------------------------------
# Vertical Pod Autoscaler
# -----------------------------------------------------------------------------
# Provides resource recommendations for workers.
# Workers have variable resource usage based on activity type.
# -----------------------------------------------------------------------------
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: pull-temporal-worker-vpa
  namespace: pull
  labels:
    app: pull-temporal-worker
  annotations:
    description: "VPA for Temporal workers - recommendation mode"
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: pull-temporal-worker

  updatePolicy:
    # Recommendations only - don't auto-update
    updateMode: "Off"

  resourcePolicy:
    containerPolicies:
      - containerName: worker
        controlledResources: ["cpu", "memory"]
        # Workers need more minimum resources
        minAllowed:
          cpu: "250m"
          memory: "512Mi"
        # Allow higher maximums for burst activities
        maxAllowed:
          cpu: "4000m"
          memory: "8Gi"
        mode: Auto
